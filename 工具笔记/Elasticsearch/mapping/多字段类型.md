# 多字段类型

## Excat Value and Full Text

* 精确值，包括数字 / 日期 / 具体一个字符串（例如 “Apple Store”）
  * Elasticsearch 中的 keyword
* 全文本，非结构化的文本数据
  * Elasticsearch 中的 text

精确值不需要做分词处理



## 自定义分词

当 Elasticsearch 自带的分词器无法满足时，可以自定义分词器，通过组合不同的组件实现

* Character Filters
  * 在 Tokenizer 之前对文本做处理，例如删除级替换字符，可以配置多个，会影响 Tokenizer 的 position 和 offset 需不需
  * 自带的 Character Filter
    * HTML strip：去除 html 标签
    * Mapping：字符串替换
    * Pattern replace：正则替换
* Tokenizers
  * 将原始的文本按照一定的规则，切分为词（term、token）
  * 自带的 Tokenizer
    * whitespace：空格
    * standard：标准
    * uax_url_email：email
    * pattern：正则
    * keyword：不处理
    * path hierarchy：文件路径
  * 可以用 Java 开发自定义的 Tokenizer
* Token Filters
  * 将 Tokenizer 输出的单词（term）进行增加，修改，删除
  * 自带的 Token Filters
    * lowercase：转小写
    * stop：停用词
    * synonym：近义词



```properties
// 对 html 标签替换
POST _analyze
{
  "tokenizer": "keyword",
  "char_filter": ["html_strip"],
  "text":"<b>hello world</b>"
}

// 字符串替换
POST _analyze
{
  "tokenizer": "standard",
  "char_filter": [{
    "type": "mapping",
    "mappings": ["- => _"]
  }],
  "text": "123-456, I-test! test-990 650-555-1234"
}

// 正则替换
POST _analyze
{
  "tokenizer": "standard",
  "char_filter": [{
    "type":"pattern_replace",
    "pattern": "http://(.*)",
    "replacement": "$1"
  }],
  "text": "http://www.elastic.co"
}

// 获取每一级目录
POST _analyze 
{
  "tokenizer": "path_hierarchy",
  "text": "/usr/local/tools"
}

// whitespace 与 stop
GET _analyze
{
  "tokenizer": "whitespace",
  "filter": ["stop"],
  "text": ["The rain in Spain falls mainly on the plain."]
}

// whitespace 与 stop
// lowercase 去除大写的词语
GET _analyze
{
  "tokenizer": "whitespace",
  "filter": ["lowercase","stop"],
  "text": ["The rain in Spain falls mainly on the plain."]
}

// 自定义
PUT my_index
{
  "settings": {
    "number_of_shards": "1",
    "number_of_replicas": 0, 
  	"analysis": {
  		"char_filter": {
  			"emoticons": {
  				"type": "mapping",
  				"mappings": [
  					":) => _happy",
  					":( => _sad"
  				]
  			}
  		},
  		"filter": {
  			"english_stop": {
  				"type": "stop",
  				"stopwords": [
  					"_english_", "an"
  				]
  			}
  		},
  		"tokenizer": {
  			"punctuation": {
  				"type": "pattern",
  				"pattern": "[ .,!?]"
  			}
  		},
  		"analyzer": {
  			"my_analyzer": {
  				"type": "custom",
  				"char_filter": [
  					"html_strip",
  					"emoticons"
  				],
  				"tokenizer": "punctuation",
  				"filter": [
  					"lowercase",
  					"english_stop"
  				]
  			}
  		}
  	}
	},
	"mappings": {
		"doc": {
			"properties": {
				"nickname": {
					"type": "text",
					"analyzer": "my_analyzer"
				}
			}
		}
	}
}
// 查看索引
GET /my_index

// 测试
POST /my_index/_analyze
{
  "analyzer":"my_analyzer",
  "text":"I'm a :) person, and you?"
}
```

